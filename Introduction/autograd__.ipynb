{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tính vi phân tự động\n",
    "\n",
    "- Vi phân là phép tính thiết yếu trong hầu như tất cả thuật toán học sâu. Mặc dù các phép toán khá trực quan nhưng với các mô hình phức tạp thì việc tự tính rõ ràng rất dễ sai.\n",
    "- Gói thư viện autograd giải quyết vấn đề này một cách nhanh chóng và hiệu quả bằng cách tự động hóa các phép dịch đạo hàm.\n",
    "- Khi đưa dữ liệu chạy qua mô hình, autograd xây dựng một đồ thị và theo dõi xem dữ liệu nào kết hợp với các phép tính nào để tạo ra kết quả. Với đồ thị này autograd sau đó có thể lan truyền ngược gradient lại theo ý muốn.\n",
    "- Lan truyền ngược ở đây chỉ là truy ngược lại đồ thị tính toán và điền vào đó các giá trị đạo hàm riêng theo từng tham số."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import autograd, np, npx\n",
    "npx.set_np()\n",
    "\n",
    "x = np.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lấy ví dụ ta cần tính vi phân của hàm số u = 2xT * x theo vector cột x.\n",
    "## Lưu ý\n",
    "1. Trước khi có thể tính gradient của y theo x ta cần nơi lưu trữ, và KHÔNG được cấp phát thêm bộ nhớ mỗi khi tính đạo hàm theo một biến xác định vì ta thường cập nhật cùng một tham số hàng vạn lần và sẽ nhanh chóng hết bộ nhớ.\n",
    "2. Bản thân giá trị gradient theo vector x cũng là một vector với cùng kích thước. Do vậy trong mã nguồn sẽ trực quan hơn nếu ta lưu giá trị gradient tính theo x dưới dạng một thuộc tính của ndarray. Ta cấp bộ nhớ cho gradient của một ndarray bằng cách gọi phương thức attach_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi tính toán gradient theo biến x, ta có thể truy cập nó thông qua thuộc tính grad. Để an toàn, x.grad được khởi tạo là một mảng chứa các giá trị 0. \n",
    "Điều này hợp lý vì trong học sâu, lấy gradient thường là để cập nhật các tham số bằng cách cộng hoặc trừ gradient để cực đại hoặc cực tiểu hóa hàm đó. Bằng cách khởi tạo gradent bằng mảng chứa giá trị 0, ta đảm bảo rằng bất kỳ cập nhật vô tình nào trước khi gradient được tính toán sẽ không làm thay đổi các giá trị của các tham số."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MXNet sẽ bật một thiết bị ghi hình để ghi lại đường đi của mỗi biến được tạo, và điều này chỉ xảy ra khi được ra lệnh rõ ràng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(28.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = 2 * np.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do x là một ndarray có độ dài bằng 4, np.dot sẽ tính toán tích vô hướng của x và x, trả về một số vô hướng rồi gán cho y. Tiếp theo, ta sẽ tính toán gradient của y theo mỗi thành phần của x một cách tự động bằng cách gọi hàm backward của y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi kiểm tra lại giá trị của x.grad, ta sẽ thấy nó được ghi đè bằng gradient mới được tính toán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient tương ứng của hàm y = 2 * Transpose(x) * x là 4x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu ta tiếp tục tính gradient của một biến khác mà giá trị của nó là kết quả theo biến x thì nội dung trong x.grad sẽ bị ghi đè."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truyền ngược cho các biến không phải số vô hướng\n",
    "- Khi y không phải số vô hướng thì vi phân của một vector y theo vector x là một ma trận.\n",
    "- Tuy nhiên, khi đối tượng này xuất hiện trong học sâu thì khi gọi làn truyền ngược trên một vector, ta đang tính toán hàm mất mát theo mỗi batch bao gồm một vài mẫu huấn luyện. Ở đây, ý định của ta không phải là tính toán ma trận vi phân mà là tính tổng của các đạo hàm riêng được tính toán độc lập cho mỗi mẫu trong batch,\n",
    "- Vì vậy, khi ta gọi backward lên một biến vector y - là một hàm của x, MXNet sẽ cho rằng ta muốn tính tổng của gradient. Tức là MXNet sẽ tạo một biến mới có giá trị là số vô hướng bằng cách cộng lại các phần tử trong y và tính gradient theo x của biến mới này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x * x # Y là một vector có số dạng tương tự với X\n",
    "y.backward()\n",
    "\n",
    "u = x.copy()\n",
    "u.attach_grad()\n",
    "\n",
    "with autograd.record():\n",
    "    v = (u * u).sum() # V là một biến vô hướng, V = Y.sum()\n",
    "v.backward()\n",
    "\n",
    "x.grad == u.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
