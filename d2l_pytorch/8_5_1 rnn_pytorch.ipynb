{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Thiết lập mạng RNN\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    # num_inputs là số lượng vocab biểu diễn bằng one-hit\n",
    "    def __init__(self, num_hidden, vocab, device):\n",
    "        super().__init__()\n",
    "        num_inputs = len(vocab)\n",
    "        num_outputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.device = device\n",
    "        self.vocab = vocab\n",
    "        self.W_xh = nn.Parameter(\n",
    "            data = torch.rand(size = (num_inputs, num_hidden), device = device, requires_grad=True)\n",
    "        )\n",
    "        self.W_hh = nn.Parameter(\n",
    "            data = torch.rand(size = (num_hidden, num_hidden), device = device, requires_grad=True)\n",
    "        )\n",
    "        self.b_h = nn.Parameter(\n",
    "            data = torch.rand(size = (1, num_hidden), device = device, requires_grad=True)\n",
    "        )\n",
    "\n",
    "        self.W_hq = nn.Parameter(\n",
    "            data = torch.rand(size = (num_hidden, num_outputs), device = device, requires_grad=True)\n",
    "        )\n",
    "        self.b_q = nn.Parameter(\n",
    "            data = torch.rand(size = (1, num_outputs), device = device, requires_grad=True)\n",
    "        )\n",
    "    \n",
    "    def init_state(self, batch_size):\n",
    "        state = torch.zeros(size = (batch_size, self.num_hidden), device = self.device)\n",
    "        state = state.detach()\n",
    "        return state\n",
    "    def forward(self, X, state = None):\n",
    "        if state is None or X.shape[0] != state.shape[0]:\n",
    "            state = self.init_state(X.shape[0])\n",
    "            \n",
    "        state = state.detach()\n",
    "        # X = X.to(self.device)\n",
    "        X = F.one_hot(X.T, num_classes=len(self.vocab)).to(device = self.device, dtype = torch.float32)\n",
    "        # X = X.to(dtype = torch.float32)\n",
    "        outputs = []\n",
    "        # Shape of X: state_number, batch_size, vocab_size\n",
    "\n",
    "        for x in X:\n",
    "            state = F.tanh(torch.mm(x, self.W_xh) + torch.mm(state, self.W_hh) + self.b_h)\n",
    "            output = torch.mm(state, self.W_hq) + self.b_q\n",
    "            outputs.append(output)  # Chỉ dùng PyTorch tensor\n",
    "\n",
    "        return torch.cat(outputs, dim=0).to(self.device), state\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "data = d2l.TimeMachine(batch_size, num_steps)\n",
    "data_iter = data.get_dataloader(train = True)\n",
    "vocab = data.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNN(num_hidden = 256, vocab = vocab, device = device)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.SGD(params = net.parameters(), lr = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient clipping\n",
    "# d2l.grad_clipping(net, theta = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15.,  8.,  0.,  9.,  6.,  0., 21., 16., 16., 12.,  0., 16., 15.,  6.,\n",
      "         0., 16.,  7.,  0., 21.,  9.,  6.,  0., 20., 14.,  2., 13., 13.,  0.,\n",
      "        16.,  4., 21.,  2.,  8., 16., 15.]) tensor([ 8,  0,  9,  6,  0, 21, 16, 16, 12,  0, 16, 15,  6,  0, 16,  7,  0, 21,\n",
      "         9,  6,  0, 20, 14,  2, 13, 13,  0, 16,  4, 21,  2,  8, 16, 15,  2])\n"
     ]
    }
   ],
   "source": [
    "for X, y in data_iter:\n",
    "    X= X.to(dtype = torch.float32)\n",
    "    print(X[0], y[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'astropppppppppp'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(net : RNN, str, num_pred):\n",
    "    outputs = [str[0]]\n",
    "    state = None\n",
    "    for i in str[1:]:\n",
    "        batch = torch.tensor(\n",
    "            data = vocab[i], device = device\n",
    "        ).reshape(1, 1)\n",
    "        _, state = net(batch, state = state)\n",
    "        outputs.append(i)\n",
    "\n",
    "    for i in range(num_pred):\n",
    "        batch = torch.tensor(vocab.token_to_idx[outputs[-1]], device = device).reshape(1, 1)\n",
    "        output, state = net(batch, state = state)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        outputs.append(vocab.idx_to_token[output.argmax(axis = -1)[0]])\n",
    "    return ''.join(outputs)\n",
    "\n",
    "predict(net, \"astro\", 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Perplexity: 1.149525016346556\n",
      "Epoch: 1 | Perplexity: 1.149511491822107\n",
      "Epoch: 2 | Perplexity: 1.149503598944623\n",
      "Epoch: 3 | Perplexity: 1.149493141021616\n",
      "Epoch: 4 | Perplexity: 1.1494854056346155\n",
      "Epoch: 5 | Perplexity: 1.149476144132992\n",
      "Epoch: 6 | Perplexity: 1.1494668566708344\n",
      "Epoch: 7 | Perplexity: 1.149460945938677\n",
      "Epoch: 8 | Perplexity: 1.1494512196799098\n",
      "Epoch: 9 | Perplexity: 1.1494451452016097\n",
      "Epoch: 10 | Perplexity: 1.149441038090221\n",
      "Epoch: 11 | Perplexity: 1.1494390286616958\n",
      "Epoch: 12 | Perplexity: 1.1494298694786331\n",
      "Epoch: 13 | Perplexity: 1.1494253908176029\n",
      "Epoch: 14 | Perplexity: 1.1494164260391373\n",
      "Epoch: 15 | Perplexity: 1.1494120486612984\n",
      "Epoch: 16 | Perplexity: 1.1494102166974203\n",
      "Epoch: 17 | Perplexity: 1.14940235581056\n",
      "Epoch: 18 | Perplexity: 1.1493993647897303\n",
      "Epoch: 19 | Perplexity: 1.149396838270256\n",
      "Epoch: 20 | Perplexity: 1.1493922279263835\n",
      "Epoch: 21 | Perplexity: 1.1493893828833581\n",
      "Epoch: 22 | Perplexity: 1.1493798895009157\n",
      "Epoch: 23 | Perplexity: 1.1493805941509254\n",
      "Epoch: 24 | Perplexity: 1.1493778033404272\n",
      "Epoch: 25 | Perplexity: 1.1493721034676876\n",
      "Epoch: 26 | Perplexity: 1.1493730848851074\n",
      "Epoch: 27 | Perplexity: 1.1493636890822203\n",
      "Epoch: 28 | Perplexity: 1.149363368193536\n",
      "Epoch: 29 | Perplexity: 1.149358672870672\n",
      "Epoch: 30 | Perplexity: 1.1493584131465253\n",
      "Epoch: 31 | Perplexity: 1.1493577696732173\n",
      "Epoch: 32 | Perplexity: 1.1493546739802876\n",
      "Epoch: 33 | Perplexity: 1.1493515963266574\n",
      "Epoch: 34 | Perplexity: 1.1493477042213587\n",
      "Epoch: 35 | Perplexity: 1.1493485144612674\n",
      "Epoch: 36 | Perplexity: 1.1493446428086784\n",
      "Epoch: 37 | Perplexity: 1.1493424794920286\n",
      "Epoch: 38 | Perplexity: 1.1493424060534378\n",
      "Epoch: 39 | Perplexity: 1.149339230770562\n",
      "Epoch: 40 | Perplexity: 1.1493362943349905\n",
      "Epoch: 41 | Perplexity: 1.1493352886157526\n",
      "Epoch: 42 | Perplexity: 1.1493337919037216\n",
      "Epoch: 43 | Perplexity: 1.1493317040731605\n",
      "Epoch: 44 | Perplexity: 1.149331969216909\n",
      "Epoch: 45 | Perplexity: 1.1493279271240933\n",
      "Epoch: 46 | Perplexity: 1.1493288943642896\n",
      "Epoch: 47 | Perplexity: 1.1493248402253904\n",
      "Epoch: 48 | Perplexity: 1.1493246196937974\n",
      "Epoch: 49 | Perplexity: 1.1493231103907404\n",
      "Epoch: 50 | Perplexity: 1.1493219438334612\n",
      "Epoch: 51 | Perplexity: 1.1493201350856173\n",
      "Epoch: 52 | Perplexity: 1.149319078522588\n",
      "Epoch: 53 | Perplexity: 1.1493169273656736\n",
      "Epoch: 54 | Perplexity: 1.1493155115677147\n",
      "Epoch: 55 | Perplexity: 1.1493125050442725\n",
      "Epoch: 56 | Perplexity: 1.1493116204614666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[249], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;66;03m# if epoch % 10 == 0:\u001b[39;00m\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperplexity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[249], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, trainer, loss_fn, train_iter, num_epoch)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(net : RNN, trainer, loss_fn, train_iter, num_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch):\n\u001b[1;32m---> 30\u001b[0m         perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;66;03m# if epoch % 10 == 0:\u001b[39;00m\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperplexity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[249], line 14\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(net, trainer, loss_fn, train_iter)\u001b[0m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, y)\n\u001b[0;32m     13\u001b[0m trainer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# for p in net.parameters():\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#     if p.grad is None:\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#         print(\"Gradient is None for:\", p.shape)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#     else:\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#         print(\"asdasdASD\")\u001b[39;00m\n\u001b[0;32m     22\u001b[0m d2l\u001b[38;5;241m.\u001b[39mgrad_clipping(net, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\d2l\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\d2l\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\d2l\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_epoch(net : RNN, trainer, loss_fn, train_iter):\n",
    "    metric = d2l.Accumulator(2)\n",
    "    state = None\n",
    "    for X, y in train_iter:\n",
    "        net.train()\n",
    "        output, state = net(X, state)\n",
    "        output = output.reshape(y.shape[0], num_steps, -1)\n",
    "        y = F.one_hot(y, num_classes=len(vocab)).float().to(output.device)\n",
    "\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        trainer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        d2l.grad_clipping(net, 1)\n",
    "        trainer.step()\n",
    "        metric.add(np.sum(loss.cpu().detach().numpy()), y.shape[0])\n",
    "    # Return perplexity per epoch\n",
    "    return np.exp(metric[0] / metric[1])\n",
    "\n",
    "def train(net : RNN, trainer, loss_fn, train_iter, num_epoch = 100):\n",
    "    for epoch in range(num_epoch):\n",
    "        perplexity = train_epoch(net, trainer, loss_fn, train_iter)\n",
    "        # if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Perplexity: {perplexity}\")\n",
    "    \n",
    "train(net, trainer, loss, data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient is None for: torch.Size([28, 256])\n",
      "Gradient is None for: torch.Size([256, 256])\n",
      "Gradient is None for: torch.Size([1, 256])\n",
      "Gradient is None for: torch.Size([256, 28])\n",
      "Gradient is None for: torch.Size([1, 28])\n"
     ]
    }
   ],
   "source": [
    "for p in net.parameters():\n",
    "    if p.grad is None:\n",
    "        print(\"Gradient is None for:\", p.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'astropppppppppp'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(net, \"astro\", 10)\n",
    "\n",
    "d2l.tra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
